{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dungeon.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zelladoor/Machine-Learning-Projects/blob/master/gpt-neo_dungeon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdtMW21T9pDl"
      },
      "source": [
        "# Instructions\n",
        "**This notebook contains two custom finetuned models called 2.7B-horni and 2.7B-horni-ln which are NOT official EleutherAI models or affiliated with them in any way beyond using their model as a basis. You can also select the original EleutherAI/gpt-neo-2.7B model in the model selection's dropdown menu.**\n",
        "\n",
        "Go through each cell in this notebook one by one, take a look at the options and descriptions and then press the play button to the left of it. You can skip the optional one. Don't skip any of the others. After running the \"Play\" cell, a small form will appear underneath, which you can use to actually play. Check out this [guide](https://gitlab.com/nolialsea/novelaicontributions/-/blob/master/Guides/Using%20finetuneanon%27s%20Colab.md) for details.\n",
        "\n",
        "To reset the state of your game, run the \"Setup\" cell again. Closing the notebook will lose your progress, so if you want to keep your story, use the \"history\" action, copy out your story to a text editor. You can also copy out your author's note and memory from the output of the \"info\" action.\n",
        "\n",
        "The most reliable way of loading the models is to store them in your google drive. This notebook will automatically download the models from a google drive in the model setup step. If this succeeds, you can then copy it into your drive in the optional following step. You can also download the files yourself and upload them to your drive yourself.\n",
        "\n",
        "* [2.7B-horni-ln](https://mega.nz/file/rQcWCTZR#tCx3Ztf_PMe6OtfgI95KweFT5fFTcMm7Nx9Jly_0wpg) [[Google Drive](https://drive.google.com/file/d/1M1JY459RBIgLghtWDRDXlD4Z5DAjjMwg/view?usp=sharing)] 5GB, for light novel styled output\n",
        "* [2.7B-horni](https://mega.nz/file/6BNykLJb#B6gxK3TnCKBpeOF1DJMXwaLc_gcTcqMS0Lhzr1SeJmc) [[Google Drive](https://drive.google.com/file/d/1-Jj_hlyNCQxuSnK7FFBXREGnRSMI5MoF/view?usp=sharing)] 5GB, for NSFW styled output\n",
        "* Torrent: magnet:?xt=urn:btih:31d956ff4a248dcf914b1b7e474cbac02d70d6a4&dn=gtp-neo-horni&tr=http%3A%2F%2Fopenbittorrent.com%3A80%2Fannounce\n",
        "* Torrent (original model):\n",
        "    * half precision mod, use this: magnet:?xt=urn:btih:f50bb4e259d2f96aa9151443950b0d2b899a097c&dn=gpt-neo-2.7B-halved&tr=http%3A%2F%2Fopenbittorrent.com%3A80%2Fannounce&tr=http%3A%2F%2Ft.nyaatracker.com%3A80%2Fannounce&tr=udp%3A%2F%2Fopen.stealth.si%3A80%2Fannounce\n",
        "    * full precision: (magnet:?xt=urn:btih:1dbaef1fc33238a732ba6aba735fb26683814860&dn=gpt-neo-2.7B&tr=http%3A%2F%2Fopenbittorrent.com%3A80%2Fannounce))\n",
        "\n",
        "Anon says about Google Drive: If you run into quotas, create a folder in your own google drive, add a shortcut to the file in question in said folder, and then download the folder itself instead of the file directly.\n",
        "\n",
        "If you have an GTX card with 8GB or more, you may be able to run this notebook locally. If you integrate the models in clover edition, use the transformers fork that's installed in the setup cell of this notebook to avoid OOM errors and use model.generate() to be able to use repetition_penalty_range and repetition_penalty_slope. For running locally, remove the map_device arguments from torch.load calls.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucP5hOdoMgzC",
        "cellView": "form"
      },
      "source": [
        "#@title Setup\n",
        "#@markdown Run this for setting up dependencies or resetting actions\n",
        "!pip install git+https://github.com/finetuneanon/transformers@gpt-neo-localattention3-rp-b\n",
        "#!wget -c http://ftp.us.debian.org/debian/pool/main/m/megatools/megatools_1.11.0~git20200404-1_amd64.deb -O megatools.deb\n",
        "#!dpkg -i megatools.deb\n",
        "!pip install gdown\n",
        "!nvidia-smi\n",
        "\n",
        "import os\n",
        "\n",
        "from transformers import GPTNeoForCausalLM, AutoTokenizer\n",
        "import tarfile\n",
        "import codecs\n",
        "import torch\n",
        "import threading\n",
        "import time\n",
        "import subprocess\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "import ipywidgets as widgets\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "\n",
        "def warn_timeout_thread():\n",
        "  global warn_time\n",
        "  while True:\n",
        "    while warn_time == -1 or time.time() < warn_time:\n",
        "      time.sleep(1)\n",
        "    warn_time = -1\n",
        "    if warn_timeout:\n",
        "      try:\n",
        "        warning_out.append_stdout(\"Ten minutes are up. Remember to rerun this cell so you don't get disconnected.\")\n",
        "      except:\n",
        "        pass\n",
        "\n",
        "try:\n",
        "  initialized += 1\n",
        "except:\n",
        "  get_ipython().events.register('pre_run_cell', set_css)\n",
        "  tail_free_sampling, top_k, top_p, temperature, number_generated_tokens, repetition_penalty, repetition_penalty_range, repetition_penalty_slope, number_show_last_actions = 0.95, 60, 0.9, 0.8, 40, 1.25, 300, 3.33, 15\n",
        "  prevent_square_brackets, prevent_angle_brackets, prevent_curly_brackets = True, True, True\n",
        "  enable_top_k, enable_top_p, enable_tfs = False, False, True\n",
        "  bad_words_ids = None\n",
        "  initialized = 0\n",
        "  last_free_edit = \"\"\n",
        "  last_prompt = \"\"\n",
        "  warn_timeout = False\n",
        "  warn_time = -1\n",
        "  threading.Thread(target=warn_timeout_thread).start()\n",
        "\n",
        "actions = []\n",
        "memory = (\"\", torch.zeros((1, 0)).long())\n",
        "lmi = [\"\", torch.zeros((1, 0)).long()]\n",
        "an = (\"\", torch.zeros((1, 0)).long())\n",
        "an_depth = 3\n",
        "history = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3ZF4eCFMV6E",
        "cellView": "form"
      },
      "source": [
        "#@title Model setup\n",
        "#@markdown horni was finetuned for one epoch on about 800MB worth of random blocks of text from literotica. Do not use the horni model if you dislike NSFW outputs. horni-ln uses horni as a base and was finetuned for one epoch on 579MB of text from a light novel dataset.\n",
        "\n",
        "print(\"Setting up model, this will take a few minutes. Don't interrupt this cell even takes a long while, or you can be left with broken, half unpacked files.\")\n",
        "\n",
        "model_name = \"2.7B-horni-ln\" #@param [\"2.7B-horni-ln\", \"2.7B-horni\", \"EleutherAI/gpt-neo-2.7B\"]\n",
        "model_gdrive = \"/content/drive/MyDrive/gpt-neo-2.7B-horni-ln.tar\" #@param {type:\"string\"}\n",
        "use_gdrive = False #@param {type:\"boolean\"}\n",
        "#@markdown If you download errors, the google drive downloads might be over their daily download quota. In that case, right-click, select \"interrupt execution\", download the checkpoint from mega yourself, upload to your google drive, tick use_gdrive and put the correct filename, e.g. `gpt-neo-2.7B-horni-ln.tar` and restart the cell.\n",
        "#@markdown\n",
        "#@markdown Warnings about certain attention bias parameters being uninitialized or about the google drive already having been mounted can be ignored.\n",
        "\n",
        "custom_models = [\"2.7B-horni\", \"2.7B-horni-ln\"]\n",
        "\n",
        "if use_gdrive:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "#model_types = {\"2.7B-horni\": \"https://mega.nz/file/6BNykLJb#B6gxK3TnCKBpeOF1DJMXwaLc_gcTcqMS0Lhzr1SeJmc\",\n",
        "#               \"2.7B-horni-ln\": \"https://mega.nz/file/rQcWCTZR#tCx3Ztf_PMe6OtfgI95KweFT5fFTcMm7Nx9Jly_0wpg\"}\n",
        "model_types = {\"2.7B-horni\": \"https://drive.google.com/uc?id=1-Jj_hlyNCQxuSnK7FFBXREGnRSMI5MoF\",\n",
        "               \"2.7B-horni-ln\": \"https://drive.google.com/uc?id=1M1JY459RBIgLghtWDRDXlD4Z5DAjjMwg\"}\n",
        "\n",
        "model = None\n",
        "tokenizer = None\n",
        "pipeline = None\n",
        "checkpoint = None\n",
        "\n",
        "if not os.path.isdir(\"gpt-neo-\"+model_name) and model_name in custom_models:\n",
        "  if use_gdrive:\n",
        "    tar = tarfile.open(model_gdrive, \"r\")\n",
        "  else:\n",
        "    model_url = model_types[model_name]\n",
        "    print(\"Downloading:\", model_url)\n",
        "    #!megadl $model_url --no-ask-password\n",
        "    !gdown $model_url\n",
        "    tar = tarfile.open(\"gpt-neo-\" + model_name + \".tar\", \"r\")\n",
        "  tar.extractall()\n",
        "  tar.close()\n",
        "\n",
        "if model_name in custom_models:\n",
        "  checkpoint = torch.load(\"gpt-neo-\" + model_name + \"/pytorch_model.bin\", map_location=\"cuda:0\")\n",
        "  model = GPTNeoForCausalLM.from_pretrained(\"gpt-neo-\" + model_name, state_dict=checkpoint).half().to(\"cuda\").eval()\n",
        "  for k in list(checkpoint.keys()):\n",
        "    del checkpoint[k]\n",
        "  del checkpoint\n",
        "else:\n",
        "  from transformers.file_utils import cached_path, WEIGHTS_NAME, hf_bucket_url\n",
        "  archive_file = hf_bucket_url(model_name, filename=WEIGHTS_NAME)\n",
        "  resolved_archive_file = cached_path(archive_file)\n",
        "  checkpoint = torch.load(resolved_archive_file, map_location=\"cuda:0\")\n",
        "  for k in checkpoint.keys():\n",
        "    checkpoint[k] = checkpoint[k].half()\n",
        "  model = GPTNeoForCausalLM.from_pretrained(model_name, state_dict=checkpoint).half().to(\"cuda\").eval()\n",
        "  for k in list(checkpoint.keys()):\n",
        "    del checkpoint[k]\n",
        "  del checkpoint\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# torch.multinomial fp16 bug is workarounded inside the transformers fork now\n",
        "#if torch.cuda.get_device_properties(0).total_memory > 15000 * 1024 * 1024:\n",
        "#  print(\"Big GPU detected, using fp32\")\n",
        "#  model = model.float()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZFSWPEP1U2b",
        "cellView": "form"
      },
      "source": [
        "#@title Copy downloaded model to google drive (optional)\n",
        "#@markdown If the model checkpoint was downloaded automatically in the previous step, you can copy it to your google drive here for more reliable access in the future\n",
        "gdrive_target = \"/content/drive/MyDrive/gpt-neo-2.7B-horni.tar\" #@param {type:\"string\"}\n",
        "copy_model_file = False #@param {type:\"boolean\"}\n",
        "\n",
        "if copy_model_file:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  model_tar = '/content/' + model_name + \".tar\"\n",
        "  !cp -v $model_tar $gdrive_target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ls_x_sSLN2hU",
        "cellView": "form"
      },
      "source": [
        "#@title Sampling settings (DO NOT SKIP)\n",
        "#@markdown You can modify sampling settings here. Don't forget to run the cell again after changing. The number of generated tokens is subtracted from the context window size, don't set it high.\n",
        "tail_free_sampling = 0.95 #@param {type:\"number\"}\n",
        "top_k = 60 #@param {type:\"number\"}\n",
        "top_p = 0.9 #@param {type:\"number\"}\n",
        "temperature =  0.8#@param {type:\"number\"}\n",
        "number_generated_tokens =  40#@param {type:\"integer\"}\n",
        "repetition_penalty = 2.5 #@param {type:\"number\"}\n",
        "repetition_penalty_range = 512 #@param {type:\"number\"}\n",
        "repetition_penalty_slope = 3.33 #@param {type:\"number\"}\n",
        "number_show_last_actions = 15 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown If tail free sampling is enabled, top_p and top_k should probably not be used.\n",
        "enable_tfs = False #@param {type:\"boolean\"}\n",
        "enable_top_k = True #@param {type:\"boolean\"}\n",
        "enable_top_p = True #@param {type:\"boolean\"}\n",
        "\n",
        "if not enable_tfs:\n",
        "  tail_free_sampling = None\n",
        "if not enable_top_k:\n",
        "  top_k = None\n",
        "if not enable_top_p:\n",
        "  top_p = None\n",
        "\n",
        "#@markdown Temperatures seem to give results different from those in AID, so play around with it. Even 0.5 can give good results."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYeGnR7uzR4d",
        "cellView": "form"
      },
      "source": [
        "#@title Prevent tokens like [, <, > and { from being generated\n",
        "#thanks STARSTRUCK\n",
        "\n",
        "prevent_square_brackets = True #@param {type:\"boolean\"}\n",
        "prevent_angle_brackets = True #@param {type:\"boolean\"}\n",
        "prevent_curly_brackets = True #@param {type:\"boolean\"}\n",
        "\n",
        "vocab = tokenizer.get_vocab()\n",
        "vocab_keys = vocab.keys()\n",
        "bad_keys = list()\n",
        "find_keys = lambda char : [key for key in vocab_keys if key.find(char) != -1]\n",
        "\n",
        "if prevent_square_brackets:\n",
        "  bad_keys.extend(find_keys(\"[\"))\n",
        "  #bad_keys.extend(find_keys(\"]\"))\n",
        "\n",
        "if prevent_angle_brackets:\n",
        "  bad_keys.extend(find_keys(\"<\"))\n",
        "  bad_keys.extend(find_keys(\">\"))\n",
        "\n",
        "if prevent_curly_brackets:\n",
        "  bad_keys.extend(find_keys(\"{\"))\n",
        "  #bad_keys.extend(find_keys(\"}\"))\n",
        "\n",
        "bad_words_ids = list()\n",
        "bad_keys_final = list()\n",
        "for key in bad_keys:\n",
        "  if key == \"<|endoftext|>\" or key in bad_keys_final:\n",
        "    continue\n",
        "  bad_id = vocab[key]\n",
        "  bad_words_ids.append([bad_id])\n",
        "  bad_keys_final.append(key)\n",
        "\n",
        "if len(bad_words_ids) < 1:\n",
        "  bad_words_ids = None\n",
        "\n",
        "#print(f\"Bad keys: {bad_keys_final} (Count: {len(bad_keys)})\")\n",
        "#print(f\"Bad ids: {bad_words_ids}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd-HLb2nXaaA",
        "cellView": "form"
      },
      "source": [
        "#@title Basic sampling\n",
        "\n",
        "#@markdown Use this cell if you just want to sample from the model in a free form way.\n",
        "\n",
        "basic_prompt = \"The rays of the evening sun falling in through the window bathed the room in a soft, warm light\" #@param {type:\"string\"}\n",
        "\n",
        "ids = tokenizer(basic_prompt, return_tensors=\"pt\").input_ids.to(\"cpu\")\n",
        "n_ids = ids.shape[1]\n",
        "if n_ids < 1:\n",
        "  n_ids = 1\n",
        "  ids = torch.tensor([[tokenizer.eos_token_id]])\n",
        "max_length = n_ids + number_generated_tokens\n",
        "torch.cuda.empty_cache()\n",
        "basic_output = model.generate(\n",
        "    ids.long().cuda(),\n",
        "    do_sample=True,\n",
        "    min_length=max_length,\n",
        "    max_length=max_length,\n",
        "    temperature=temperature,\n",
        "    tfs = tail_free_sampling,\n",
        "    top_k = top_k,\n",
        "    top_p = top_p,\n",
        "    repetition_penalty = repetition_penalty,\n",
        "    repetition_penalty_range = repetition_penalty_range,\n",
        "    repetition_penalty_slope = repetition_penalty_slope,\n",
        "    use_cache=True,\n",
        "    bad_words_ids=bad_words_ids,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ").long().to(\"cpu\")\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(tokenizer.decode(basic_output[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GKfy1uQ9QdU"
      },
      "source": [
        "# Using the play function\n",
        "\n",
        "If your prompt starts with a letter, try putting a space or newline in front.\n",
        "\n",
        "* **generate** adds your prompt as an action and generates more output\n",
        "* **continue** generates more output\n",
        "* **edit** copies the last action into the prompt field and sets action to replace\n",
        "* **replace** replaces the last output with the prompt and generates more, use this to edit\n",
        "* **info** outputs LMI and memory\n",
        "* **history** outputs all actions so far\n",
        "* **memory** sets memory to the text in the prompt field\n",
        "* **authorsnote** sets author's note to the text in the prompt field\n",
        "* **andepth** sets the depth of the author's note to the number in the prompt\n",
        "* **tokenize** tokenizes the text in the prompt field and outputs the number of tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb_NHHISOeYg",
        "cellView": "form"
      },
      "source": [
        "#@title Play\n",
        "\n",
        "#@markdown Free edit gives you a text field that you can freely edit instead of a static output field, but it also limits retry and undo functionality. When warn_timeout is enabled, a warning will be displayed after 10 minutes, reminding you to rerun this cell so you don't get disconnected.\n",
        "\n",
        "warn_timeout = True #@param {type:\"boolean\"}\n",
        "free_edit_mode = True #@param {type:\"boolean\"}\n",
        "\n",
        "action_type = \"generate\"\n",
        "prompt = \"\"\n",
        "need_refresh = True\n",
        "dirty = True\n",
        "last_chars = 0\n",
        "\n",
        "action_types = [\"generate\", \"continue\", \"edit\", \"replace\", \"undo\", \"retry\", \"memory\", \"authorsnote\", \"andepth\", \"info\", \"history\", \"tokenize\"]\n",
        "\n",
        "def assemble():\n",
        "  global actions\n",
        "  remaining = (2048 - number_generated_tokens + 1) - memory[1].shape[1] - an[1].shape[1]\n",
        "  if free_edit_mode:\n",
        "    lines = free_edit.value.splitlines(keepends=True)\n",
        "    actions = [(line, tokenizer(line, return_tensors=\"pt\").input_ids.to(\"cpu\")) for line in lines]\n",
        "  n_actions = len(actions)\n",
        "  n_ctx = 0\n",
        "  back_i = n_actions\n",
        "  for i in range(n_actions):\n",
        "      i_action = n_actions - i - 1\n",
        "      n_tok = actions[i_action][1].shape[1]\n",
        "      if remaining > n_ctx + n_tok:\n",
        "        n_ctx += n_tok\n",
        "        back_i = i_action\n",
        "      else:\n",
        "        break\n",
        "  lmi[0], lmi[1] = memory[0], memory[1]\n",
        "  start = False\n",
        "  if n_actions - back_i - 1 < an_depth:\n",
        "    start = True\n",
        "  while back_i < n_actions:\n",
        "    if start or n_actions - back_i - 1 == an_depth:\n",
        "      lmi[0] += an[0]\n",
        "      lmi[1] = torch.cat([lmi[1].cpu(), an[1].cpu()], 1).long()\n",
        "      start = False\n",
        "    lmi[0] += actions[back_i][0]\n",
        "    lmi[1] = torch.cat([lmi[1].cpu(), actions[back_i][1].cpu()], 1).long()\n",
        "    back_i += 1\n",
        "\n",
        "def clear_output():\n",
        "  with out:\n",
        "    IPython.display.clear_output()\n",
        "\n",
        "def set_action(change):\n",
        "  global action_type\n",
        "  action_type = change.new\n",
        "\n",
        "def set_free_edit(change):\n",
        "  global dirty, last_free_edit\n",
        "  dirty = True\n",
        "  last_free_edit = change.new\n",
        "\n",
        "def set_prompt(change):\n",
        "  global prompt, last_prompt\n",
        "  prompt = change.new\n",
        "  last_prompt = prompt\n",
        "\n",
        "@torch.no_grad()\n",
        "def play(do_action=None):\n",
        "  global memory, need_refresh, an, an_depth, action_type, history, dirty, last_chars, last_free_edit, last_prompt\n",
        "  warning_out.append_stdout(\"\")\n",
        "  an_updated = False\n",
        "  memory_updated = False\n",
        "  if free_edit_mode:\n",
        "    clear_output()\n",
        "  if do_action is not None:\n",
        "    action = do_action\n",
        "    action_type = do_action\n",
        "  else:\n",
        "    action = action_type\n",
        "  with out:\n",
        "    if prompt in action_types:\n",
        "      action == prompt\n",
        "    else:\n",
        "      if action == \"edit\":\n",
        "        if len(actions) > 0:\n",
        "          input.value = actions[-1][0]\n",
        "        else:\n",
        "          input.value = \"\"\n",
        "        dropdown.value = \"replace\"\n",
        "        return\n",
        "      if action == \"replace\":\n",
        "        if len(actions) > 0:\n",
        "          actions.pop()\n",
        "        need_refresh = True\n",
        "        action = \"generate\"\n",
        "      if action == \"generate\":\n",
        "        text = prompt\n",
        "        if len(text) > 0:\n",
        "          for line in text.splitlines(True):\n",
        "            tokens = tokenizer(line, return_tensors=\"pt\").input_ids.to(\"cpu\")\n",
        "            actions.append((line, tokens))\n",
        "        action = \"continue\"\n",
        "      if action == \"info\":\n",
        "        clear_output()\n",
        "        print(\"LMI: \" + lmi[0])\n",
        "        print(\"LMI tokens: \" + str(lmi[1].shape[1]))\n",
        "        print(\"Memory: \" + memory[0])\n",
        "        print(\"Author's note: \" + an[0])\n",
        "        print(\"Author's note depth: \" + str(an_depth))\n",
        "        need_refresh = True\n",
        "      if action == \"history\":\n",
        "        clear_output()\n",
        "        print(\"\".join([action[0] for action in actions]), end=\"\")\n",
        "        need_refresh = False\n",
        "      if action == \"retry\":\n",
        "        if free_edit_mode:\n",
        "          if dirty:\n",
        "            print(\"Text was edited, can't retry.\")\n",
        "            return\n",
        "          free_edit.value = free_edit.value[:-last_chars]\n",
        "        else:\n",
        "          if len(actions) > 0:\n",
        "            actions.pop()\n",
        "        need_refresh = True\n",
        "        action = \"continue\"\n",
        "      if action == \"undo\":\n",
        "        if free_edit_mode:\n",
        "          if dirty:\n",
        "            print(\"Text was edited, can't undo.\")\n",
        "          else:\n",
        "            dirty = True\n",
        "            free_edit.value = free_edit.value[:-last_chars]\n",
        "          return\n",
        "        if len(actions) > 0:\n",
        "          actions.pop()\n",
        "        assemble()\n",
        "        clear_output()\n",
        "        print(\"\".join([action[0] for action in actions[-number_show_last_actions:]]), end=\"\")\n",
        "        need_refresh = False\n",
        "      if action == \"memory\":\n",
        "        if prompt == \"\":\n",
        "          memory = (\"\", torch.zeros((1, 0)).long())\n",
        "          text = \"\"\n",
        "        else:\n",
        "          text = codecs.decode(prompt + \"\\n\", \"unicode-escape\")\n",
        "          tokens = tokenizer(text, return_tensors=\"pt\").input_ids.to(\"cpu\")\n",
        "          memory = (text, tokens)\n",
        "        clear_output()\n",
        "        print(\"Memory: \" + text)\n",
        "        memory_updated = True\n",
        "      if action == \"authorsnote\":\n",
        "        if prompt == \"\":\n",
        "          an = (\"\", torch.zeros((1, 0)).long())\n",
        "          text = \"\"\n",
        "        else:\n",
        "          text = \"\\n[Author's note: \" + codecs.decode(prompt, \"unicode-escape\") + \"]\\n\"\n",
        "          tokens = tokenizer(text, return_tensors=\"pt\").input_ids.to(\"cpu\")\n",
        "          an = (text, tokens)\n",
        "        clear_output()\n",
        "        print(\"Author's note: \" + text)\n",
        "        an_updated = True\n",
        "      if action == \"andepth\":\n",
        "        clear_output()\n",
        "        try:\n",
        "          an_depth = int(codecs.decode(prompt + \"\\n\", \"unicode-escape\"))\n",
        "        except:\n",
        "          pass\n",
        "        print(\"Author's note depth: \" + str(an_depth))\n",
        "        an_updated = True\n",
        "      if action == \"tokenize\":\n",
        "        text = codecs.decode(prompt, \"unicode-escape\")\n",
        "        tokens = tokenizer(text, return_tensors=\"pt\").input_ids.to(\"cpu\")\n",
        "        clear_output()\n",
        "        print(\"Tokens: \" + str(tokens.shape[1]))\n",
        "        print(tokens[0])\n",
        "        need_refresh = True\n",
        "      if action == \"continue\":\n",
        "        assemble()\n",
        "        ids = lmi[1].cuda()\n",
        "        n_ids = ids.shape[1]\n",
        "        if n_ids < 1:\n",
        "          n_ids = 1\n",
        "          ids = torch.tensor([[tokenizer.eos_token_id]])\n",
        "        max_length = number_generated_tokens + n_ids\n",
        "        #ids[:, :] = 13\n",
        "        torch.cuda.empty_cache()\n",
        "        clear_output()\n",
        "        gen_tokens = model.generate(\n",
        "            ids.long().cuda(),\n",
        "            do_sample=True,\n",
        "            min_length=max_length,\n",
        "            max_length=max_length,\n",
        "            temperature=temperature,\n",
        "            tfs = tail_free_sampling,\n",
        "            top_k = top_k,\n",
        "            top_p = top_p,\n",
        "            repetition_penalty = repetition_penalty,\n",
        "            repetition_penalty_range = repetition_penalty_range,\n",
        "            repetition_penalty_slope = repetition_penalty_slope,\n",
        "            use_cache=True,\n",
        "            bad_words_ids=bad_words_ids,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        ).long()\n",
        "        stop_tokens = [0, 13, 30, 526, 764, 1701, 2474, 5145, 5633]\n",
        "        for i in reversed(range(len(gen_tokens[0]))):\n",
        "          if i < n_ids:\n",
        "            gen_tokens = gen_tokens[0]\n",
        "            break\n",
        "          if gen_tokens[0][i] in stop_tokens:\n",
        "            gen_tokens = gen_tokens[0][:i+1]\n",
        "            break\n",
        "        gen_text = tokenizer.decode(gen_tokens[n_ids:])\n",
        "        last_chars = len(gen_text)\n",
        "        if last_chars > 0:\n",
        "          actions.append((gen_text, gen_tokens[n_ids:].unsqueeze(0).cpu()))\n",
        "        if free_edit_mode:\n",
        "          free_edit.value = \"\".join([action[0] for action in actions])\n",
        "          dirty = False\n",
        "        else:\n",
        "          print(\"\".join([action[0] for action in actions[-number_show_last_actions:]]), end=\"\")\n",
        "        torch.cuda.empty_cache()\n",
        "        need_refresh = False\n",
        "    if history is not None:\n",
        "      if history:\n",
        "        with out_history:\n",
        "          IPython.display.clear_output()\n",
        "          print(\"\".join([action[0] for action in actions]), end=\"\")\n",
        "        with out_history2:\n",
        "          IPython.display.clear_output()\n",
        "      else:\n",
        "        with out_history2:\n",
        "          IPython.display.clear_output()\n",
        "          print(\"\".join([action[0] for action in actions]), end=\"\")\n",
        "        with out_history:\n",
        "          IPython.display.clear_output()\n",
        "      if an_updated:\n",
        "        with out_an:\n",
        "          IPython.display.clear_output()\n",
        "          if len(an[0]) > 0:\n",
        "            print(\"AN depth: \" + str(an_depth) + \"\\n\" + an[0], end=\"\")\n",
        "      if memory_updated:\n",
        "        with out_memory:\n",
        "          IPython.display.clear_output()\n",
        "          print(memory[0], end=\"\")\n",
        "      history = not history\n",
        "    last_free_edit = free_edit.value\n",
        "    last_prompt = input.value\n",
        "\n",
        "import ipywidgets as widgets\n",
        "import IPython.display\n",
        "out = widgets.Output(layout={'border': '1px solid black', \"width\": \"1280px\"})\n",
        "dropdown = widgets.Dropdown(options=action_types, value=action_type, description='Action:', disabled=False)\n",
        "dropdown.observe(set_action, 'value')\n",
        "button = widgets.Button(description='[selected action]', disabled=False)\n",
        "button.on_click(lambda _: play(dropdown.value))\n",
        "generate_button = widgets.Button(description='Generate', disabled=False)\n",
        "generate_button.on_click(lambda _: play(\"generate\"))\n",
        "edit_button = widgets.Button(description='Edit', disabled=False)\n",
        "edit_button.on_click(lambda _: play(\"edit\"))\n",
        "retry_button = widgets.Button(description='Retry', disabled=False)\n",
        "retry_button.on_click(lambda _: play(\"retry\"))\n",
        "continue_button = widgets.Button(description='Continue', disabled=False)\n",
        "continue_button.on_click(lambda _: play(\"continue\"))\n",
        "undo_button = widgets.Button(description='Undo', disabled=False)\n",
        "undo_button.on_click(lambda _: play(\"undo\"))\n",
        "dropdown_hbox = widgets.HBox([dropdown, button])\n",
        "if free_edit_mode:\n",
        "  hbox = widgets.HBox([retry_button, continue_button, undo_button])\n",
        "else:\n",
        "  hbox = widgets.HBox([generate_button, edit_button, retry_button, continue_button, undo_button])\n",
        "input = widgets.Textarea(value='', placeholder='', description='Input:', disabled=False, rows=4, layout={\"width\": \"1280px\"})\n",
        "input.observe(set_prompt, 'value')\n",
        "free_edit = widgets.Textarea(value='', placeholder='', description='Story:', disabled=False, rows=25, layout={\"width\": \"1280px\"})\n",
        "free_edit.observe(set_free_edit, 'value')\n",
        "\n",
        "warning_out = widgets.Output(layout={'border': '0px solid white', \"width\": \"1280px\"})\n",
        "warn_time = time.time() + 600\n",
        "\n",
        "if free_edit_mode:\n",
        "  display(free_edit, dropdown_hbox, warning_out, hbox, input, out)\n",
        "  free_edit.value = last_free_edit#\"\".join([action[0] for action in actions])\n",
        "  input.value = last_prompt\n",
        "else:\n",
        "  display(out, dropdown_hbox, warning_out, hbox, input)\n",
        "  with out:\n",
        "    print(\"\".join([action[0] for action in actions[-number_show_last_actions:]]), end=\"\")\n",
        "  free_edit.value = last_free_edit\n",
        "  input.value = last_prompt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "jPtYoUNO39Mg"
      },
      "source": [
        "#@title History\n",
        "#@markdown Run this cell to have an auto-updating full listing of the current story.\n",
        "\n",
        "history = True\n",
        "out_history = widgets.Output(layout={'border': '1px solid black', \"width\": \"1280px\"})\n",
        "out_history2 = widgets.Output(layout={'border': '1px solid black', \"width\": \"1280px\"})\n",
        "out_memory = widgets.Output(layout={'border': '1px solid black', \"width\": \"1280px\"})\n",
        "out_an = widgets.Output(layout={'border': '1px solid black', \"width\": \"1280px\"})\n",
        "display(out_history, out_history2, out_memory, out_an)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvIvzStC9rY9",
        "cellView": "form"
      },
      "source": [
        "#@title Attention display\n",
        "#@markdown If you don't know what this is, just ignore. attn_head_combination selects the operation used to combine layer and attention head results.\n",
        "\n",
        "visualizer_prompt = \"Before his eyes was an orange cat with stripes. Running his fingers through its soft fur, he admired\" #@param {type:\"string\"}\n",
        "max_attentions =  8#@param {type:\"integer\"}\n",
        "attn_head_combination = \"mean\" #@param [\"mean\", \"max\"]\n",
        "use_lmi_as_prompt = False #@param {type:\"boolean\"}\n",
        "\n",
        "if use_lmi_as_prompt:\n",
        "  ids = lmi[1]\n",
        "else:\n",
        "  ids = tokenizer(visualizer_prompt, return_tensors=\"pt\").input_ids.to(\"cpu\")\n",
        "\n",
        "n_ids = ids.shape[1]\n",
        "if n_ids < 1:\n",
        "  n_ids = 1\n",
        "  ids = torch.tensor([[tokenizer.eos_token_id]])\n",
        "\n",
        "max_length = n_ids + number_generated_tokens\n",
        "torch.cuda.empty_cache()\n",
        "basic_output = model.generate(\n",
        "    ids.long().cuda(),\n",
        "    do_sample=True,\n",
        "    min_length=max_length,\n",
        "    max_length=max_length,\n",
        "    temperature=temperature,\n",
        "    tfs = tail_free_sampling,\n",
        "    top_k = top_k,\n",
        "    top_p = top_p,\n",
        "    repetition_penalty = repetition_penalty,\n",
        "    repetition_penalty_range = repetition_penalty_range,\n",
        "    repetition_penalty_slope = repetition_penalty_slope,\n",
        "    use_cache=True,\n",
        "    bad_words_ids=bad_words_ids,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    return_dict_in_generate=True,\n",
        "    output_attentions=True\n",
        ")\n",
        "torch.cuda.empty_cache()\n",
        "attentions = basic_output[\"attentions\"]\n",
        "basic_output = basic_output[\"sequences\"].cpu()\n",
        "\n",
        "print(\"Prompt: \" + visualizer_prompt)\n",
        "print()\n",
        "\n",
        "def combine(attentions):\n",
        "  if attn_head_combination == \"mean\":\n",
        "    attentions = attentions.mean(0)\n",
        "  else:\n",
        "    attentions = attentions.max(0)[0]\n",
        "  return attentions\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "#gen_tokens x layers x tensor (1 x heads x n_ids square)\n",
        "torch.set_printoptions(sci_mode=False)\n",
        "for i in range(number_generated_tokens):\n",
        "  layer_attn = []\n",
        "  for j in range(0,len(attentions[i])):\n",
        "    layer_attn.append(combine(attentions[i][j][0,:,-1].float().cpu()))\n",
        "  layer_attn = torch.stack(layer_attn)\n",
        "  token_attn = combine(layer_attn)\n",
        "  prob, topk = token_attn.topk(max_attentions)\n",
        "  top_tokens = []\n",
        "  for top in topk:\n",
        "    decoded = tokenizer.decode(torch.tensor([basic_output[0][top]]))\n",
        "    top_tokens.append(f\"{decoded!r}@{top} ({token_attn[top]:.4f})\")\n",
        "  print(\"Token: \" + repr(tokenizer.decode(torch.tensor([basic_output[0][n_ids + i]]))))\n",
        "  print(\", \".join(top_tokens))\n",
        "  print()\n",
        "\n",
        "print(\"Output: \" + tokenizer.decode(basic_output[0]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}